#!/usr/bin/env python3
"""
å¾®ä¿¡å…¬ä¼—å·æ–‡ç« æŠ“å–è„šæœ¬
é€šè¿‡æ¨¡æ‹Ÿå¾®ä¿¡å®¢æˆ·ç«¯ User-Agent ç»•è¿‡åçˆ¬æœºåˆ¶

ç”¨æ³•ï¼š
    python fetch_wechat_article.py <å…¬ä¼—å·æ–‡ç« é“¾æ¥> [--download-images]
    python fetch_wechat_article.py <é“¾æ¥1> <é“¾æ¥2> ...  # æ‰¹é‡å¤„ç†

ç¤ºä¾‹ï¼š
    python fetch_wechat_article.py "https://mp.weixin.qq.com/s/xxx"
    python fetch_wechat_article.py "https://mp.weixin.qq.com/s/xxx" --download-images
"""

import sys
import re
import html
import subprocess
import os
import tempfile
import json
from pathlib import Path
from datetime import datetime


# å¾®ä¿¡å®¢æˆ·ç«¯ User-Agent
WECHAT_UA = "Mozilla/5.0 (Linux; Android 13; V2148A) AppleWebKit/537.36 Chrome/116.0.0.0 Mobile Safari/537.36 MicroMessenger/8.0.49.2600 WeChat/arm64 Weixin NetType/WIFI Language/zh_CN"


def fetch_wechat_article(url: str) -> dict:
    """æŠ“å–å¾®ä¿¡å…¬ä¼—å·æ–‡ç« å†…å®¹"""

    # ä½¿ç”¨ curl å‘é€è¯·æ±‚ï¼ˆæ›´ç¨³å®šï¼‰
    result_proc = subprocess.run(
        ["curl", "-s", "-L", "-A", WECHAT_UA, url],
        capture_output=True,
        text=True,
        timeout=30,
    )
    content = result_proc.stdout

    result = {
        "url": url,
        "title": "",
        "author": "",
        "description": "",
        "content": "",
        "images": [],
        "is_video": False,
        "raw_html_length": len(content),
    }

    # æå–æ ‡é¢˜ - ä½¿ç”¨æ›´å®½æ¾çš„æ­£åˆ™
    title_match = re.search(r"msg_title = window\.title = ['\"]([^'\"]+)['\"]", content)
    if title_match:
        result["title"] = html.unescape(title_match.group(1).replace("&amp;", "&"))
    else:
        # å¤‡ç”¨æ–¹æ¡ˆï¼šä» og:title æå–
        title_match = re.search(r'property="og:title" content="([^"]+)"', content)
        if title_match:
            result["title"] = html.unescape(title_match.group(1))

    # æå–æè¿°
    desc_match = re.search(r'name="description" content="([^"]+)"', content)
    if desc_match:
        desc = desc_match.group(1)
        # å¤„ç†è½¬ä¹‰å­—ç¬¦
        desc = desc.replace("\\x0a", "\n").replace("\\x26", "&").replace("&amp;", "&")
        result["description"] = html.unescape(desc)

    # åˆ¤æ–­æ˜¯å¦ä¸ºè§†é¢‘å·æ–‡ç« ï¼ˆæ£€æŸ¥å®é™…çš„ h1 æ ‡ç­¾ï¼Œè€Œä¸æ˜¯ JS ä»£ç ï¼‰
    if re.search(r'<h1[^>]*id="js_video_page_title"', content):
        result["is_video"] = True
        result["content"] = result["description"]
    else:
        # æ™®é€šå›¾æ–‡æ–‡ç« ï¼šä» js_content æå–å®Œæ•´æ­£æ–‡
        content_match = re.search(r'id="js_content"[^>]*>(.*?)</div>\s*</div>\s*</div>', content, re.DOTALL)
        if content_match:
            inner = content_match.group(1)
            # å»é™¤ HTML æ ‡ç­¾ï¼Œä¿ç•™æ–‡æœ¬
            clean = re.sub(r'<[^>]+>', '\n', inner)
            clean = html.unescape(clean)
            # æ¸…ç†å¤šä½™ç©ºç™½
            lines = [line.strip() for line in clean.split('\n') if line.strip()]
            result["content"] = '\n'.join(lines)
        else:
            result["content"] = result["description"]

    # æå–å…¬ä¼—å·åç§°
    author_match = re.search(r"nick_name: JsDecode\(['\"]([^'\"]+)['\"]\)", content)
    if author_match:
        result["author"] = author_match.group(1)
    else:
        # å¤‡ç”¨æ–¹æ¡ˆï¼šä»é“¾æ¥æ–‡æœ¬æå–
        author_match = re.search(r'class="account_nickname_inner">([^<]+)<', content)
        if author_match:
            result["author"] = author_match.group(1).strip()

    # æå–å›¾ç‰‡é“¾æ¥ï¼ˆdata-src å’Œ src ä¸¤ç§æ–¹å¼ï¼‰
    images = set()
    for pattern in [r'data-src="(https://mmbiz\.qpic\.cn[^"]+)"', r'src="(https://mmbiz\.qpic\.cn[^"]+)"']:
        for match in re.finditer(pattern, content):
            img_url = match.group(1).replace("&amp;", "&")
            images.add(img_url)
    result["images"] = sorted(list(images))

    return result


def download_images(images: list, output_dir: str = None) -> list:
    """ä¸‹è½½å›¾ç‰‡åˆ°æŒ‡å®šç›®å½•ï¼Œè¿”å›æœ¬åœ°æ–‡ä»¶è·¯å¾„åˆ—è¡¨"""
    if not images:
        return []

    if output_dir is None:
        output_dir = tempfile.mkdtemp(prefix="wechat_article_")

    Path(output_dir).mkdir(parents=True, exist_ok=True)

    downloaded = []
    for i, img_url in enumerate(images, 1):
        # ç¡®å®šæ–‡ä»¶æ‰©å±•å
        if "wx_fmt=gif" in img_url:
            ext = "gif"
        elif "wx_fmt=png" in img_url:
            ext = "png"
        else:
            ext = "jpg"

        filename = f"img_{i:02d}.{ext}"
        filepath = os.path.join(output_dir, filename)

        # ä½¿ç”¨ curl ä¸‹è½½
        result = subprocess.run(
            ["curl", "-s", "-o", filepath, img_url],
            capture_output=True,
            timeout=30,
        )

        if result.returncode == 0 and os.path.exists(filepath) and os.path.getsize(filepath) > 0:
            downloaded.append(filepath)
        # è¿›åº¦æ¶ˆæ¯åœ¨è°ƒç”¨å¤„æ ¹æ®éœ€è¦æ‰“å°

    return downloaded


def fetch_multiple_articles(urls: list) -> list:
    """æ‰¹é‡æŠ“å–å¤šç¯‡æ–‡ç« """
    results = []
    for i, url in enumerate(urls, 1):
        print(f"\nğŸ“„ æ­£åœ¨æŠ“å–ç¬¬ {i}/{len(urls)} ç¯‡...")
        try:
            article = fetch_wechat_article(url)
            results.append(article)
            print(f"   âœ… {article['title'][:30]}...")
        except Exception as e:
            print(f"   âŒ æŠ“å–å¤±è´¥: {e}")
            results.append({"url": url, "error": str(e)})
    return results


def output_json(article: dict):
    """è¾“å‡º JSON æ ¼å¼ï¼ˆä¾›å…¶ä»–ç¨‹åºè°ƒç”¨ï¼‰"""
    print(json.dumps(article, ensure_ascii=False, indent=2))


def output_summary(article: dict, image_paths: list = None):
    """è¾“å‡ºç»“æ„åŒ–æ€»ç»“"""
    print("=" * 50)
    print(f"ã€æ ‡é¢˜ã€‘{article['title']}")
    print(f"ã€ä½œè€…ã€‘{article['author']}")
    print(f"ã€ç±»å‹ã€‘{'è§†é¢‘å·æ–‡ç« ' if article['is_video'] else 'å›¾æ–‡æ–‡ç« '}")
    print(f"ã€é…å›¾æ•°é‡ã€‘{len(article['images'])} å¼ ")
    print("=" * 50)
    print("ã€æ­£æ–‡ã€‘")
    print(article["content"])
    print("=" * 50)

    if article["images"]:
        print("ã€é…å›¾é“¾æ¥ã€‘")
        for i, img in enumerate(article["images"][:10], 1):
            print(f"  {i}. {img[:80]}...")
        if len(article["images"]) > 10:
            print(f"  ... å…± {len(article['images'])} å¼ ")

    if image_paths:
        print("=" * 50)
        print("ã€å·²ä¸‹è½½å›¾ç‰‡ã€‘")
        for path in image_paths:
            print(f"  ğŸ“· {path}")

    print("=" * 50)


def output_markdown(article: dict, image_paths: list = None):
    """è¾“å‡º Markdown æ ¼å¼ï¼Œé€‚åˆå­˜æ¡£å’Œè¿›ä¸€æ­¥å¤„ç†"""
    article_type = "è§†é¢‘å·æ–‡ç« " if article["is_video"] else "å›¾æ–‡æ–‡ç« "

    md = f"""# {article['title']}

## åŸºæœ¬ä¿¡æ¯

| é¡¹ç›® | å†…å®¹ |
|------|------|
| **ä½œè€…** | {article['author']} |
| **ç±»å‹** | {article_type} |
| **é…å›¾** | {len(article['images'])} å¼  |
| **æ¥æº** | [åŸæ–‡é“¾æ¥]({article['url']}) |

---

## æ­£æ–‡

{article['content']}

---

## é…å›¾

"""
    if image_paths:
        for i, path in enumerate(image_paths, 1):
            md += f"- å›¾{i}: `{path}`\n"
    elif article["images"]:
        for i, img in enumerate(article["images"], 1):
            md += f"- å›¾{i}: {img}\n"
    else:
        md += "*æ— é…å›¾*\n"

    md += """
---

## å¾…æ€»ç»“ï¼ˆç”± cc å¡«å†™ï¼‰

### æ ¸å¿ƒè§‚ç‚¹
1.
2.
3.

### å…³é”®ä¿¡æ¯
-
-

### é‡‘å¥æ‘˜å½•
> ""
> ""

### æ€è€ƒ/è¿­ä»£ç‚¹
- å¯¹æˆ‘æœ‰ä»€ä¹ˆå¯å‘ï¼Ÿ
- æœ‰ä»€ä¹ˆå¯ä»¥å€Ÿé‰´çš„ï¼Ÿ

---

*æŠ“å–æ—¶é—´: """ + datetime.now().strftime("%Y-%m-%d %H:%M") + "*\n"

    print(md)


def main():
    if len(sys.argv) < 2:
        print("ç”¨æ³•:")
        print("  python fetch_wechat_article.py <å…¬ä¼—å·æ–‡ç« é“¾æ¥>")
        print("  python fetch_wechat_article.py <é“¾æ¥> --download-images")
        print("  python fetch_wechat_article.py <é“¾æ¥> --json")
        print("  python fetch_wechat_article.py <é“¾æ¥> --markdown")
        print("  python fetch_wechat_article.py <é“¾æ¥1> <é“¾æ¥2> ...  # æ‰¹é‡å¤„ç†")
        sys.exit(1)

    # è§£æå‚æ•°
    args = sys.argv[1:]
    download_flag = "--download-images" in args
    json_flag = "--json" in args
    markdown_flag = "--markdown" in args

    urls = [arg for arg in args if arg.startswith("http")]

    try:
        if len(urls) == 1:
            # å•ç¯‡æ–‡ç« 
            article = fetch_wechat_article(urls[0])

            image_paths = None
            if download_flag:
                if not json_flag:
                    print("ğŸ“¥ æ­£åœ¨ä¸‹è½½é…å›¾...")
                image_paths = download_images(article["images"])
                if not json_flag:
                    for path in image_paths:
                        print(f"  âœ… ä¸‹è½½æˆåŠŸ: {os.path.basename(path)}")
                article["downloaded_images"] = image_paths

            if json_flag:
                output_json(article)
            elif markdown_flag:
                output_markdown(article, image_paths)
            else:
                output_summary(article, image_paths)

        elif len(urls) > 1:
            # æ‰¹é‡å¤„ç†
            articles = fetch_multiple_articles(urls)

            if json_flag:
                print(json.dumps(articles, ensure_ascii=False, indent=2))
            else:
                print("\n" + "=" * 50)
                print(f"ğŸ“š æ‰¹é‡æŠ“å–å®Œæˆï¼Œå…± {len(articles)} ç¯‡")
                print("=" * 50)
                for i, article in enumerate(articles, 1):
                    if "error" in article:
                        print(f"\nâŒ æ–‡ç«  {i}: æŠ“å–å¤±è´¥ - {article['error']}")
                    else:
                        print(f"\nğŸ“„ æ–‡ç«  {i}: {article['title']}")
                        print(f"   ä½œè€…: {article['author']}")
                        print(f"   é…å›¾: {len(article['images'])} å¼ ")
        else:
            print("é”™è¯¯ï¼šè¯·æä¾›è‡³å°‘ä¸€ä¸ªå…¬ä¼—å·æ–‡ç« é“¾æ¥")
            sys.exit(1)

    except Exception as e:
        print(f"æŠ“å–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
